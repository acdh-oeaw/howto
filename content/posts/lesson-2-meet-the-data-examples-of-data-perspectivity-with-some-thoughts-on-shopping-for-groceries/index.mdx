---
title: "Lesson 2: Meet the data. Examples of data perspectivity, with some
  thoughts on shopping for groceries."
lang: en
date: 2022-11-15T00:00:00.000Z
version: 1.0.0
authors:
  - oberreither-bernhard
tags:
  - semantic-web
abstract: " "
licence: ccby-4.0
toc: false
uuid: NMAwVSgOmXKRyGQd1HfTg
shortTitle: "Lesson 2: Meet the data"
---
_– Two days in –_

#### Data sources

I'm having a debrief with Peter to come back to an issue briefly discussed in our first meeting: the source data. At first glance, it might look somewhat homogeneous – it’s all text indices and indices of persons –, but this impression fades as soon as one takes a closer look. Here’s a short list:

##### Data source 1: “Die Fackel online”.

- A relational database with names and basic biographical data of about 16,900 persons, as well as links to 129,000 mentions of said persons in “Die Fackel”.
- An XML file containing a text index of the approx. 14,000 texts published in “Die Fackel”, modeled in a custom standard.
- A transcription of “Die Fackel” consisting of more than 22,000 pages as XML files also modeled in a custom standard, containing the 129,000 mentions of persons.

##### Data source 2: “Dritte Walpurgisnacht”.

- A TEI-XML file containing an index of both persons and texts, literary and journalistic, mentioned / quoted in “Dritte Walpurgisnacht” (1933, published posthumous), with basic biographical resp. standard bibliographical data.
- Approx. 3,000 mentions of / quotes from said persons and texts annotated in a TEI-XML containing the full text of “Dritte Walpurgisnacht”.

##### Data source 3: “Karl Kraus Rechtsakten”.

- Two TEI-XML files containing bibliographical data of the texts mentioned or quoted in Kraus’ legal files.
- A TEI-XML file containing an index of persons mentioned in Kraus’ legal papers.
- More than 4,000 TEI-XML files containing transcriptions of Kraus’ files from over 200 legal cases with bibliographical metadata in their TEI headers, as well as mentions of persons and mentions of resp. quotes in the body of the files.

Now, where some might see a nightmare of endless integration problems, others might see a task cut out perfectly for a modeling based on an ontology with as wide a scope as CIDOC crm (which will serve as the main vocabulary for our data model).

#### Data perspectivity

But before we can seriously start to model it, we first need to understand it – since the scope and function of any given data model is not self-evident, and even less so the meaning of any given data field in its respective data set. Every data set is the result of a modeling process, which, as, e.g., Elena Pierazzo has pointed out, implies

- it's the result of a selection process
- that simplifies its object domain to some degree
- and that is very much guided by a specific interest 
- and a certain perspective necessarily lacking self-evidence. [^1]

These traits are inherent to every data set. If you’ve never done that kind of work, you might not have come across this issue and the problems it entails may not be immediately tangible. It helps to think of grocery lists. Not the ones you write for yourself, but those written by someone else, and you’re the one supposed to pick up the items. These lists do not convey objective or complete information. So much detail is left out, so many terms are abbreviated, possibly encrypted. That list that looks so innocent by itself – the moment you have to act on it, the moment you have to put your understanding of the data to the test, it turns into an abyss of ambiguity and senselessness. Think of the chill running down your spine when you're standing in the dairy aisle, the elevated pulse. That’s what working with someone else’s data is like.

  In this regard, the SemanticKraus project admittedly is in a highly privileged position: All of our three source projects are at close vicinity, their coordinators and maintainers are just next door, in part literally. Also, one of them is me. If we don’t understand the data and we don’t find an explanation in a documentation somewhere (or no documentation at all), we can still ask or go through our own notes.

  To give an example of our data’s perspectivity: A simple issue that might come up would be that in the Dritte Walpurgisnacht project, all biographical data is selected with a clear focus on the year 1933 (see here for our editorial note on the matter). After all, Dritte Walpurgisnacht is, among other things, an account of what’s happened on a day-to-day basis in 1933, informed by uncounted newspaper articles quoted and woven into the text, and in this particular year, people’s biographies – to put in very general terms – took abrupt turns. This focus makes sense from a reader’s (and thus from the editor’s) point of view, but is not self-evident from the data as represented in the XML file. 

  Another – perhaps even more particular – matter is related to the bibliographical entries: See for example these three entries, which seemingly denote the same kind of concept – texts:

```
<bibl xml:id="DWbibl01944" sortKey="Ludwig_Emil">
  <author>Emil Ludwig</author>
  <title level="m">Geschenke des Lebens</title>
  <title level="m" type="subtitle">Ein Rückblick</title>
  <pubPlace>Berlin</pubPlace>
  <date>1931</date>
  <citedRange xml:id="DWbibl01945">S. 291–292</citedRange>
</bibl>

<bibl xml:id="DWbibl00505" sortKey="Schiller_Friedrich_von">
  <author>Friedrich von Schiller</author>
  <title level="m">Das Lied von der Glocke</title>
  <date>1799</date>
  <citedRange xml:id="DWbibl01705">Vers 379–382</citedRange> 
</bibl>

<bibl xml:id="DWbibl03941" sortKey="N._N.">
  <title level="a">Fahnenverbot – noch strengere Polizeistrafen</title>
  <title level="a" type="subtitle">Maßnahmen gegen die 
    Arbeiterkonsumvereine – Beschlüsse des Ministerrates</title>
  <title level="j">Arbeiter-Zeitung</title>
  <title level="j" type="short">AZ</title>
  <pubPlace>Wien</pubPlace>
  <date>20. 5. 1933</date>
  <biblScope>S. 1</biblScope>
  <citedRange xml:id="DWbibl03940"/>
</bibl>
```

These are two literary texts and one newspaper article, in a pretty ordinary modeling. Still, a lot can be said about why this modeling was implemented, about which academic routines and standards played a role, which technical and methodological aspects were taken into account, and, last but not least, which aspects of the data were neglected. To make it short: We wanted to provide the data for – and allow the display of – specific citation styles.
Some differentiations did not matter in this respect: For example, for the purposes of the project it was not a problem that a citedRange-element, while usually giving a page number of a referred to passage, could mean different things when used as an empty tag: It could mean that the ‘cited range’ actually covered the whole referred to text, which in traditional citation styles is usually indicated by not referring to any of the text’s subdivisions (e.g., pages). At the same time, the empty tag can indicate that it referred to a part of the text but, for some reason, no subdivision of the referred to text in smaller units was available – the most trivial reason being that the text itself does not exceed one page (see the third example).

These subtle differences come into play when we need to ask the question: What would this data look like when seen through the lens of our new data model? Which entities would need to be modeled in each of the three cases? The newspaper article, e.g., is a text within a larger publication within another larger publication, so to speak, so there’s (at least) three entities to take into account: the text, the issue, and the newspaper or periodical itself. Also, the seemingly small distinction between a text with and without a specified edition will have a major role to play as we go forward creating a data model. All that’s implicit in our data, and also some of what’s not in our data, will need to be considered.

Anyway, back to my meeting with Peter. While we briefly mention the other data, our main focus is on articles.xml, an XML file at the heart of our project, containing the complete table of contents of Die Fackel: a periodical that ran for 37 years, in 415 issues (922 numbers), on over 22,000 pages. It’s a large file, to say the least, and it presents another kind of problem with regard to data perspectivity.

```
<entry displayCat="sub" nm="" parid="pr135615" fn="FK-26-668_n0096.xml" 
  type="title" pageCnt="0">
  <author></author>
  <title fn="FK-26-668_n0096.xml" titleSource="title" displayCat="sub" 
    divParID="pr135615">Sprachschule</title>
  <page>96</page>
  <publDate publDay_n="" publMonth_n="12" publYear_n="1924" publDay_a=""
    publMonth_a="12" publYear_a="1924">DEZEMBER 1924</publDate>
</entry>
```

The example shows the encoding of one entry, a text called _Sprachschule_ from the 1924 issue no. 668. The encoding is not TEI; instead, it is a custom standard that was in turn based on another standard that was developed in one of the predecessor institutions of the ACDH-CH respectively the ACE a long time ago. Reconstructing the intended meaning and function of these elements, attributes, and values is as much an archaeological endeavor as it is a logical one, but it is manageable.

However, in one respect articles.xml is a quite different type of data source than your regular index, catalogue, or database – simply because it was never intended to store data. Its use is purely functional, it provides the basis of the digital table of contents of _Die Fackel online_ (where this enormous file is displayed in conveniently sized portions, as you can see when clicking on “contents”). What we want from it is a bibliography of texts published in this periodical. However, it was not designed to deliver this bibliography, but rather to guide a reader through the sometimes impenetrable textual thicket that is _Die Fackel_. The table of contents, in its essence, provides a reader with entry points into a set of texts – the bibliography we strive for gives the metadata for each text, including its position and scope. While we aim for some degree of completeness, with a certain disregard for any ‘reader experience’, the latter is the focus of a table of contents. The effect of this difference in intended use might not be visible at first glance, and might not amount to a large number of changes, but it still requires sorting through this list by hand.

The obvious advantage of the file is – well, it’s there, it’s digital, and it’s structured data that to some 95 percent fulfills our requirements. For now, Peter generates a CSV from articles.xml using Python scripts. On the one hand, the script returns exactly the info from the source document that we need for further work – the info that will ultimately be modeled in RDF – and on the other hand the script enriches the individual entries with an important additional info (the "to" page numbers of the respective articles). Then he makes spreadsheets out of it, which look nice and are awaiting their further, manual corrections. They will be turned into google sheets to be worked on collaboratively, a task taken on by our intrepid project team members Johanna Unterholzner and Astrid Hauer  – whose work will be the subject of another blog entry.

[^1]: Pierazzo, E. (2019), 'How subjective is your model?', in Jannidis, F., and Flanders, J., (eds), _The Shape of Data in Digital Humanities. Modeling Texts and Text-based Resources._ Abingdon, Oxfordshire, and New York: Routledge, pp. 117-132. 